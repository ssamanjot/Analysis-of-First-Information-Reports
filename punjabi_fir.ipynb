{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>First_Information_Contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(es):\\n ਇਸ ਵਕਤ ਦਰਜ ਹੈ ਕਿ ਇਕ ਮਿਆਨ ਵੱਲੋਂ ਮੇਜਰ ਸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>( ):\\n ਬਿਆਨ ਅਜਾਨੇ ਸੀ ਹਸਨੈਨ S ਮੁਹੰਮਦ ਪਟਵਾਰੀ ਕੰਮ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                         First_Information_Contents\n",
       "0           0   (es):\\n ਇਸ ਵਕਤ ਦਰਜ ਹੈ ਕਿ ਇਕ ਮਿਆਨ ਵੱਲੋਂ ਮੇਜਰ ਸ...\n",
       "1           1  ( ):\\n ਬਿਆਨ ਅਜਾਨੇ ਸੀ ਹਸਨੈਨ S ਮੁਹੰਮਦ ਪਟਵਾਰੀ ਕੰਮ..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "#nltk.download('stopwords')\n",
    "data = pd.read_csv(\"FIRDf.csv\") \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=data['First_Information_Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (es):\\n ਇਸ ਵਕਤ ਦਰਜ ਹੈ ਕਿ ਇਕ ਮਿਆਨ ਵੱਲੋਂ ਮੇਜਰ ਸਿੰਘ ਪੁੱਤਰ ਲੇਟ ਬਾਰਾ ਸਿੰਘ ਕੋਮ ਜੱਟ ਸਿੱਖ ਵਾਸੀ ਮਕਾਨ ਨੰਬਰ 4250 ਛੋਸ-2 ਅਰਬਨ ਅਸਟੇਟ ਪਟਿਆਲਾ ਜਿਲਾ ਪਟਿਆਲਾ ਉਮਰ ਕਰੀਬ 77 ਬਰਖਿਲਾਫ ਅਣਪਛਾਤੇ ਵਹਿਕਲ ਦੇ ਨਾ-ਮਾਲੂਮ ਡਰਾਇਵਰ ਮੁਤਵਾ ਸ: ਕੁਲਦੀਪ ਸਿੰਘ 1919 ਥਾਣਾ ਅਰਬਨ ਅਸਟੇਟ ਪਟਿਆਲਾ ਬਰਾਏ ਦਾਇਰੀ ਮੁਕੱਦਮਾ ਹੱਥੀ ਸਿਪਾਹੀ ਰਵਿੰਦਰ ਸਿੰਘ 2152 ਦੇ ਥਾਣਾ ਮੈਸੂਲ ਹੋਇਆ ਜਿਸਦਾ ਮਜਬੂਨ ਜੋਲ ਹੈ ਬਿਆਨ ਕੀਤਾ ਕਿ ਮੈ ਉਕਤ ਪਤੇ ਦਾ ਰਹਿਣ ਵਾਲਾ ਹੈ ਅਤੇ ਮੈਂ ਗੁਰੂ ਨਾਨਕ ਦੇਵ ਯੂਨੀਵਰਸਿਟੀ ਅਸਰ ਤੋਂ ਬਰ ਪ੍ਰੋਫੈਸਰ ਰਿਟਾਇਰ ਹੋਇਆ ਹਾ ਮੇਰਾ ਰਿਸ਼ਤੇਦਾਰ ਦਲੀਪ ਸਿੰਘ ਪੁੱਤਰ ਮੱਲ ਸਿੰਘ ਉਮਰ ਕਰੀਬ 80 ਸਾਲ ਜੋ ਕਿ ਮਕਾਨ ਨੰਬਰ 18 ਖਾਲਸਾ ਕਾਲਜ ਕਲੋਨੀ ਪਟਿਆਲਾ ਰਹਿੰਦਾ ਹੈ ਜੋ ਕਿ ਯੂਨੀਵਰਸਿਟੀ ਪਟਿਆਲਾ ਤੇ ਭੋਰ ਪ੍ਰੋਫੈਸਰ ਰਿਟਾਇਰ ਹੋਏ ਹਨ ਜਿਹਨਾ ਦੀ ਪਤਨੀ 2025 ਸਾਲ ਪਹਿਲਾ ਵੈੱਬ ਹੋ ਗਈ ਸੀ ਇਹਨਾ ਦੀ ਇੱਕੀ ਬੇਟੀ ਮਿੱਠੀ ਅਲੋਂ ਬੇਟਾ ਸੰਦੀਪ ਸਿੰਘ ਉਰਫ ਸੰਨੀ ਦੇ ਮਾਹਰਲੇ ਮੁਲਕ ਯੂ.ਐਸ.ਏ ਰਹਿੰਦੇ ਹਨ ਅੱਜ ਮੈਂ ਸਵੇਰ ਆਪਣੇ ਰਿਸ਼ਤੇਦਾਰ ਦਲੀਪ ਸਿੰਘ ਪੁੱਤਰ ਮੁੱਲ ਸਿੰਘ ਵਾਸੀ ਉਕਤ ਨਾਲ ਪੰਜਾਬੀ ਯੂਨੀਵਰਿਸਟੀ ਪਟਿਆਲਾ ਤੋਂ ਮੈਂ ਆਪਣੀ ਸਕੂਟੀ ਕਨੈਟਿਕ ਪੈਂਡਾ ਨੰਬਰੀ ਪੀ ਬੀ 02 ਜੀ 7162 ਪਰ ਅਤੇ ਦਲੀਪ ਸਿੰਘ ਮੇਰੇ ਅੱਗੇ ਅੱਗੇ ਆਪਣੇ ਸਕੂਟਰ ਨੰਬਰ ਪੀ ਬੀ 11 ਕੇ 3866 ਮਾਰਕਾ ਬਜਾਜ ਚਿੰਤਕ ਪਰ ਸਵਾਰ ਹੋ ਕੇ ਕਿਸੇ ਕੰਮ ਸਬੰਧੀ ਪਟਿਆਲਾ ਸ਼ਹਿਰ ਨੂੰ ਜਾ ਰਹੇ ਸੀ ਮੈ ਦਲੀਪ ਸਿੰਘ ਤੋਂ ਵੀ ਪਿੱਛੇ ਸੀ ਵਕਤ ਕਰੀਬ 10:30 ਏ ਐਮ ਦਾ ਹੋਵੇਗਾ ਜਦੋ ਅਸੀ ਫੇਸ2 ਅਰਬਨ ਅਸਟੇਟ ਦੇ ਪੁੱਲ ਉਪਰੋ ਥੱਲੇ ਨੂੰ ਉਤਰੇ ਤਾ ਬਾਈਪਾਸ ਚੌਕ ਤੋਂ ਥੋੜਾ ਪਿਛੇ ਕਿਸੇ ਅਣਪਛਾਤੇ ਵਹਿਕਲ ਦੀ ਪਿੱਛੇ ਫੋਟ ਵੱਜਣ ਕਾਰਣ ਦਲੀਪ ਸਿੰਘ ਆਪਣੇ ਸਕੂਟਰ ਸਮੇਤ ਸੜਕ ਪਰ ਗਿਰ ਗਿਆ ਸੀ ਅਤੇ ਇਤਨੇ ਵਿੱਚ ਮੈਂ ਵੀ ਮੌਕਾ ਪਰ ਪਹੁੰਚ ਗਿਆ ਮੈਂ ਅਤੇ ਇੱਕ ਹੋਰ ਵਿਅਕਤੀ ਕੁਲਵਿੰਦਰ ਸਿੰਘ ਸਿੱਧੂ ਜੈ ਕਿ ਵਕੀਲ ਹੈ ਉਸਨੂੰ ਹੋਰ ਰਾਹਗੀਰਾਂ ਦੀ ਮਦਦ ਨਾਲ ਚੁੱਕ ਦੇ ਨੇੜੇ ਵਰਧਮਾਨ ਹਸਪਤਾਲ ਵਿੱਚ ਦਾਖਲ ਕਰਵਾ ਦਿੱਤਾ ਸੀ ਇਹ ਹਾਦਸਾ ਕਿਸੇ ਅਣਪਛਾਤੇ ਵਹਿਕਲ ਦੀ ਫੇਟ ਵੱਜਣ ਕਾਰਣ ਵਾਪਰਿਆ ਹੈ ਦਲੀਪ ਸਿੰਘ ਦੀ ਚੌਰਾਨੇ ਇਲਾਜ ਵਰਧਮਾਨ ਹਸਪਤਾਲ ਵਿੱਚ ਮੌਤ ਹੋ ਗਈ ਹੈ ਆਪ ਜੀ ਨੂੰ ਬਿਆਨ ਲਿਖਾ ਦਿੱਤਾ ਹੈ ਪੜ ਲਿਆ ਸੁਣ ਲਿਆ ਨੀਕ ਹੈ ਸਹੀ/ ਮੇਜਰ ਸਿੰਘ ਤਸਦੀਕ ਕੁਲਦੀਪ ਸਿੰਘ ਝ ਥਾਣਾ ਅਰਬਨ ਅਸਟੇਟ ਪਟਿਆਲਾ ਕਾਰਵਾਈ ਪੁਲਿਸ:- ਅੱਜ ਇੱਕ ਇਤਲਾਹ ਵੱਲ ਡਾ: ਸਰਬਜੀਤ ਕੌਰ ਵਰਧਮਾਨ ਮਹਾਵੀਰ ਹਸਪਤਾਲ ਪਟਿਆਲਾ ਤੋਂ ਮੋਸੁਲ ਹੋਈ ਕਿ ਮਜਹੂਬ ਦਲੀਪ ਸਿੰਘ ਪੁੱਤਰ ਮੁੱਲ ਸਿੰਘ ਉਮਰ ਕਰੀਬ 80 ਸਾਲ ਵਾਸੀ ਮਕਾਨ ਨੰਬਰ 18 ਖਾਲਸਾ ਕਲੋਨੀ ਪਟਿਆਲਾ ਰੋਡ ਸਾਇਡ ਐਕਸੀਡੈਂਟ ਹੋਣ ਕਾਰਨ ਇਲਾਜ ਲਈ ਦਾਖਲ ਹੋਇਆ ਹੈ ਕਾਰਵਾਈ ਲਈ ਆਈ.ਓ ਭੇਜੇ ਜਿਸ ਪਰ ਮਨ ਸ:ਧ: ਸਮੇਤ ਹੋਲਦਾਰ ਗੁਰਚਰਨ ਸਿੰਘ 293 ਅਤੇ ਸਿਪਾਹੀ ਰਵਿੰਦਰ ਸਿੰਘ 2152 ਦੇ ਬਰਾਏ ਕਰਨੇ ਕਾਰਵਾਈ ਵਰਧਮਾਨ ਮਹਾਵੀਰ ਹਸਪਤਾਲ ਪਟਿਆਲਾ ਪੁੱਜਾ ਡਾਕਟਰ ਜਿਖੇ ਮਜਬੂਬ ਦਲੀਪ ਸਿੰਘ ਦਾ ਬਿਆਨ ਲਿਖਣ ਲਈ ਡਾਕਟਰ ਸਾਹਿਬ ਦੇ ਲਿਖਤੀ ਦਰਖਾਸਤ ਪੇਸ਼ ਕੀਤੀ ਜੋ ਡਾਕਟਰ ਸਾਹਿਬ ਨੇ ਆਪਣੀ ਰਿਪੋਰਟ ਵਿੱਚ ਲਿਖਿਆ ਕਿ ਮੁਜਰੂਬ ਦਲੀਪ ਸਿੰਘ ਦੀ ਦੋਰਾਨੇ ਇਲਾਜ ਮੌਤ ਹੋ ਗਈ ਹੈ ਹਸਪਤਾਲ ਵਿੱਚ ਮ੍ਰਿਤਕ ਦਲੀਪ ਸਿੰਘ ਦਾ ਰਿਸਤੇਦਾਰ ਮੇਜਰ ਸਿੰਘ ਪੁੱਤਰ ਲੈਵ ਬਾਰਾ ਸਿੰਘ ਕੌਮ ਜੱਟ ਸਿੱਖ ਵਾਸੀ ਮਕਾਨ ਨੰਬਰ 4250 ਕੇਸ 2 ਅਰਬਨ ਅਸਟੇਟ ਪਟਿਆਲਾ ਮਲਾਕੀ ਹੋਇਆ ਜਿਸਨੇ ਮਨ ੧:੧ ਪਾਸ ਆਪਣਾ ਉਕਤ ਬਿਆਨ ਤਹਿਰੀਰ ਕਰਾਇਆ ਜੋ ਹਰਵ ਬਾ ਹਰਫ ਲਿਖਾ ਜਾ ਕਰ ਪੜ ਕਰ ਸੁਣਾਇਆ ਗਿਆ ਜਿਸ ਨੇ ਸੁਣ ਕੇ ਸਹੀ ਮੰਨ ਕੇ ਬਿਆਨ ਹੇਠ ਆਪਣੇ ਅੰਗਰੇਜੀ ਵਿੱਚ ਦਸਤਖਤ ਕੀਤੇ ਤੇ ਜਿਹਨਾ ਦੀ ਤਸਦੀਕ ਮਨ ਸਬ ਨੇ ਕੀਤੀ ਬਿਆਨਬਾਲਾ ਤੇ ਮ੍ਰਿਤਕ ਦਲੀਪ ਸਿੰਘ ਦੀ ਮੌਤ ਕਿ ਅਣਪਛਾਤੇ ਵਹਿਕਲ ਦੀ ਛੋਟ ਵੱਜਣ ਕਾਰਣ ਹਸਪਤਾਲ ਵਿੱਚ ਦੋਰਾਨੇ ਇਲਾਜ'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['First_Information_Contents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( ):\\n ਬਿਆਨ ਅਜਾਨੇ ਸੀ ਹਸਨੈਨ S ਮੁਹੰਮਦ ਪਟਵਾਰੀ ਕੰਮ ਮੁਸਲਮਾਨ ਪਿੰਡ ਫੁਲਵਣ ਪਾਰਟ ਨੂੰ 5 ਥਾਣਾ ਬਹਾਦਰ ਜੱਜ ਜ਼ਿਲਾ ਕਿਸਨਰਜ ਵਿਹਾਰ ਹਾਲ ਵਾਸੀ ਖੱਖੜਾ ਰੋੜ ਕੋਸ਼ਿਲ ਮੁਹੱਲਾ Ps ਸਿਟੀ ਫਗਵਾੜਾ ਜਿਲਾ ਕਪੂਰਥਲਾ ਉਮਰ ਕਰੀਬ 25 ਸਾਲ 9779872755 ਬਿਆਨ ਕੀਤਾ ਕਿ ਮੈਂ ਉਕਤ ਪੱਤੇ ਦਾ ਰਹਿਣ ਵਾਲਾ ਹਾਂ ਤੇ ਟਾਇਲ ਪੱਥਰ ਕਾ ਕਾਮ ਕਰਤਾ ਹੀ ਸੁਭਾ ਆਪਣੇ ਮੋਟਸਾਈਕਲ ਨੇ PB-08-CN-8854 ਪਰ ਸਵਾਰ ਹੋ ਕੇ ਵਾਇਲ ਪੱਧਰ ਕਾ ਕਾਮ ਕਰਕੇ ਫਗਵਾੜਾ ਨੇ ਲੁਧਿਆਣਾ ਆ ਰਿਹਾ ਸਾਂ। ਮੋਟਰਸਾਈਕਲ ਕੇ ਪੀਛੇ ਸਾਦਿਲ S/o ਰਵਏ ਕੰਮ ਮੁਸਲਮਾਨ ਵਾਸੀ ਪਿੰਡ ਦੱਪ ਬਲੇ ਬਾਣਾ ਦੀਗਲ ਬੈਂਕ ਜਿਲਾ ਕਿਸ਼ਨਗੰਜ ਬਿਹਾਰ ਹਾਲ ਵਾਸੀ ਖੋਥੜਾ ਮੁਹੱਲਾ PS ਸਿਟੀ ਫਗਵਾੜਾ ਜੋ ਹਮ ਏਕ ਸਾਥ ਹੀ ਰਹਿਤੇ ਖੇ, ਮੋਟਰਸਾਈਕਲ ਕੇ ਪੀਛੇ ਬੈਠਾ ਥਾ ਔਰ ਮੈਂ ਮੋਟਰਸਾਈਕਲ ਚਲਾ ਰਹਾ ਸ਼ਾ।ਪੀਛੇ ਸੇ ਏਕ ਮਰੂਤੀ ਕਾਰ ਜਿਸ ਕਾ ਨੰਬਰ ਮੁਝੇ ਯਾਦ ਨਹੀਂ ਹੈ।ਜੋ ਹਮਾਰੇ ਪੁੱਛੇ ਸੌ ਤੇਜ ਰਫਰ ਸੇ ਆਇਆ ਔਰ ਹਮੇਂ ਟੱਕਰ ਮਾਰ ਦੀ ਇਸ ਟੱਕਰ ਮੋ ਮੇਰੀ ਬਾਈ ਨਾਤ ਟੁਟ ਗਈ ਔਰ ਸਾਦਿਲ ਕੇ ਸਿਰ ਮੇਂ ਬਹੁਤ ਗਹਿਰੀ ਚੋਟ ਆਈ।ਕਾਜ਼ ਵਾਲੇ ਨੂੰ ਹਮੇ ਆਪਣੀ ਕਾਰ ਮੈਂ ਬਿਨਾਇਆ ਔਰ ਸਿਵਲ ਹਸਪਤਾਲ ਵਿਲੇਜ਼ ਲੈ ਗਿਆ ਤਾਂ ਡਾਕਟਰ ਨੇ ਮੇਰੀ ਮਲਮ ਪੱਟੀ ਕੇ ਅੱਚ ਹਮ ਦੋਨੋਂ ਕੇ ਐਬੁਲੈਂਸ ਮੈਂ ਬਿਨਾ ਕਰ ਹਾਂ DMC ਹਸਪਤਾਲ LDH ਵਿੱਚ ਦਾਖਲ ਕਰਵਾ ਦੀਆ।ਇੱਥੇ ਮੈਂ ਜੋਰ ਇਲਾਜ ਹਾਂ!ਮਰੂਚੀ ਕਾਰ ਕਾ ਨੱਥਰ ਸੁਭਾ ਪਤਾ ਕਰਕੇ ਆਪ ਕੋ ਬਤਾ ਦੇ ਮਰੂਤੀ ਕਾਰ ਵਾਲੇ ਦੇ ਖਿਲਾਫ ਕਾਨੂੰਨੀ ਕਾਰਵਾਈ ਕੀ ਜਾਏ। ਬਿਆਨ ਲਿਖਵਾ ਦੀਆ ਹੈ। Sd ਹਿੰਦੀ ਹਸਨੈਨ ਤਸਦੀਕ Sd ਪੰਜਾਬੀ ਨਿਰਮਲ ਸਿੰਘ ASI ਖਾਣਾ ਫਿਲੋਰ 06.01.2019 ਕਾਰਵਾਈ ਪੁਲਿਸ- ਅੱਜ ਮਨ AS ਹਾਜਰ ਬਾਣਾ ਹਾਂ ਕਿ MHC ਰਾਣਾ ਨੇ ਨੋਟ ਕਰਵਾਇਆ ਕਿ DMC ਲੁਧਿਆਣਾ ਤੋਂ ਇੱਕ ਫੈਨ ਮੋਸੂਲ ਹੋਇਆ ਕਿ ਦਿਲ ਜੋ ਰੰਡਮੋਕਸੀਫੈਟ ਹੋਣ ਕਰਕੇ ਸਿਰ ਦੀ ਸੱਟ ਕਾਰਨ ਜੈਜ਼ ਇਲਾਜ ਦੀ ਮੌਤ ਹੋ ਚੁੱਕੀ ਹੈ ਹਸਪਤਾਲ ਪੁੱਜ ਕੇ ਕਾਰਵਾਈ ਕੀਤੀ ਜਾਵੇ ਜਿਸਤੇ ਮਨ ASI ਸਮੇਤ HC ਸੰਜੀਵ ਕੁਮਾਰ, 417 PHG ਨਵੀਰ ਸਿੰਘ ਨੇ 2072 ਦੇ ਬਚਾਏ ਕਰਨੇ ਕਾਰਵਾਈ DMC ਹਸਪਤਾਲ ਲੁਧਿਆਣਾ ਪੁੱਜ ਕੇ ਚੁੱਕਾ ਬਰ 56876 ਮਿਤੀ 06.01.19 ਹਾਸਿਲ ਕੀਤਾ Dr. ਸਾਜ਼ਿਬ ਪਾਸ ਮਜਬੂਬ ਦਾ ਬਿਆਨ ਲੈਣ ਸਬੰਧੀ ਲਿਖਤੀ ਰਾਇ ਹਾਸਲ ਕੀਤੀ Dr. ਸਾਹਿਬ ਨੇ ਮਜਹੂ ਨੂੰ ਬਿਆਨ ਦੇਣ ਦੇ ਛਿੱਟ ਲਿਖਿਆ ਜਿਸਤੇ ਹਸਨੈਨ ਨੇ ਮੇਰੇ ਪਾਸ ਆਪਣਾ ਉਕਤ ਬਿਆਨ ਦਰਜ ਕਰਵਾਇਆ ਬਿਆਨ ਲਿਖਕੇ ਪੜਕੇ ਸੁਣਾਇਆ ਹੈ ਸਮਝਾਇਆ ਗਿਆ ਜਿਸਨੇ ਆਪਣੇ ਬਿਆਨ ਨੂੰ ਠੀਕ ਮੰਨਤ ਤੇ ਹਿੰਦੀ ਵਿੱਚ ਆਪਣੇ ਦਸਤਖਤ ਕੀਤੇ ਜਿਸਦੀ ਮਨ ASI ਨੇ ਤਸਦੀਕ ਕੀਤੀ ਬਿਆਨ ਬਾਲਾਂ ਦੇ ਜੁਰਮ 304,279, 338,337 ਕੁਦ ਦਾ ਹੋਣਾ ਪਾਇਆ ਜਾਂਦਾ ਹੈ ਜਿਸਚੇ ਚੁੱਕਾ ਹਜ਼ਾ ਲਿਖਕੇ ਮੁਕੱਦਮਾ ਦਰਜ ਰਜਿਸਟਰ ਕਰਨ ਲਈ ਹੱਥੀ PHG ਧੰਨਵੀਰ ਸਿੰਘ 282 ਦੇ ਥਾਣਾ ਭਜਿਆ ਜਾਂਦਾ ਹੈ ਮੁਕੱਦਮਾ ਦਰਜ ਕਰਕੇ ਨੰਬਰ ਮੁਕੱਦਮਾ ਤੇ ਜਾ ਕੀਤਾ ਜਾਵੇ ਕਟਲ ਰੂਮ ਪਰ ਬਜਰੀਆ ਵਾਲਿਰਲੈਸ ਇਤਲਾਹ ਦਿੱਤੀ ਜਾਵੇ ਮਨ AS। ਸਮੇਤ ਸਾਥੀ ਕਰਮਚਾਰੀ ਦੇ ਮੌਕਾ ਪਰ ਮਸ਼ਰੂਫ ਭਵਤੀ ਹਾਂ Sd ਪੰਜਾਬੀ ਨਿਰਮਲ ਸਿੰਘ ASI ਬਾਣਾ ਫਿਲੋਰ ਜਲੰਧਰ ਦਿਹਾਤੀ 06.01.19 ਅੱਜ ਸ਼ਾਹੱਦ ਰਕਬਾ DMC ਹਸਪਤਾਲ ਲੁਧਿਆਣਾ AT 10:40 PM Longitude:75,784626, Latitude: 31.030583 ਅੱਜ ਖਾਣਾ:-ਉਪਰੋਕਤ ਲਿਖ ਦੇ ਜਾਣਾ ਮਮੂਲ ਹੋਣ ਤੇ ਇਹ ਪਹਿਲੀ ਸੂਚਨਾ ਰਿਪੋਰਟ ਉਕਤ ਧਾਰਾ ਹੇਨ ਦਰਜ ਰਜਿਸਟਰ ਕਰਕੇ ਅਸਲ ਲਿਖਤ ਮਹਿ ਥਲ FIR ਹੱਥੀ PHG ਅਰਿੰਦਾ ਨਿਯਦ ASI ਪਾਸ਼ ਬਰ ਮੌਕਾ ਏ ਫੜੀ ਭੇਜੀ ਜਾ ਰਹੀ ਹੈ। ਮੁੱਖ ਅਫਸਰ ਥਾਣਾ ਅਤੇ ਕੰਟਰੋਲ ਰੂਮ ਪਰ ਬਜ਼ਰੀਆ ਵਾਇਰਲੇਸ ਇਤਲਾਹ ਭੇਜੀ ਜਾ ਰਹੀ ਹੈ।'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['First_Information_Contents'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test'] = data[\"First_Information_Contents\"].apply(lambda x : [word for word in x.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(',\n",
       " '):\\n',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਅਜਾਨੇ',\n",
       " 'ਸੀ',\n",
       " 'ਹਸਨੈਨ',\n",
       " 'S',\n",
       " 'ਮੁਹੰਮਦ',\n",
       " 'ਪਟਵਾਰੀ',\n",
       " 'ਕੰਮ',\n",
       " 'ਮੁਸਲਮਾਨ',\n",
       " 'ਪਿੰਡ',\n",
       " 'ਫੁਲਵਣ',\n",
       " 'ਪਾਰਟ',\n",
       " 'ਨੂੰ',\n",
       " '5',\n",
       " 'ਥਾਣਾ',\n",
       " 'ਬਹਾਦਰ',\n",
       " 'ਜੱਜ',\n",
       " 'ਜ਼ਿਲਾ',\n",
       " 'ਕਿਸਨਰਜ',\n",
       " 'ਵਿਹਾਰ',\n",
       " 'ਹਾਲ',\n",
       " 'ਵਾਸੀ',\n",
       " 'ਖੱਖੜਾ',\n",
       " 'ਰੋੜ',\n",
       " 'ਕੋਸ਼ਿਲ',\n",
       " 'ਮੁਹੱਲਾ',\n",
       " 'Ps',\n",
       " 'ਸਿਟੀ',\n",
       " 'ਫਗਵਾੜਾ',\n",
       " 'ਜਿਲਾ',\n",
       " 'ਕਪੂਰਥਲਾ',\n",
       " 'ਉਮਰ',\n",
       " 'ਕਰੀਬ',\n",
       " '25',\n",
       " 'ਸਾਲ',\n",
       " '9779872755',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਕੀਤਾ',\n",
       " 'ਕਿ',\n",
       " 'ਮੈਂ',\n",
       " 'ਉਕਤ',\n",
       " 'ਪੱਤੇ',\n",
       " 'ਦਾ',\n",
       " 'ਰਹਿਣ',\n",
       " 'ਵਾਲਾ',\n",
       " 'ਹਾਂ',\n",
       " 'ਤੇ',\n",
       " 'ਟਾਇਲ',\n",
       " 'ਪੱਥਰ',\n",
       " 'ਕਾ',\n",
       " 'ਕਾਮ',\n",
       " 'ਕਰਤਾ',\n",
       " 'ਹੀ',\n",
       " 'ਸੁਭਾ',\n",
       " 'ਆਪਣੇ',\n",
       " 'ਮੋਟਸਾਈਕਲ',\n",
       " 'ਨੇ',\n",
       " 'PB-08-CN-8854',\n",
       " 'ਪਰ',\n",
       " 'ਸਵਾਰ',\n",
       " 'ਹੋ',\n",
       " 'ਕੇ',\n",
       " 'ਵਾਇਲ',\n",
       " 'ਪੱਧਰ',\n",
       " 'ਕਾ',\n",
       " 'ਕਾਮ',\n",
       " 'ਕਰਕੇ',\n",
       " 'ਫਗਵਾੜਾ',\n",
       " 'ਨੇ',\n",
       " 'ਲੁਧਿਆਣਾ',\n",
       " 'ਆ',\n",
       " 'ਰਿਹਾ',\n",
       " 'ਸਾਂ।',\n",
       " 'ਮੋਟਰਸਾਈਕਲ',\n",
       " 'ਕੇ',\n",
       " 'ਪੀਛੇ',\n",
       " 'ਸਾਦਿਲ',\n",
       " 'S/o',\n",
       " 'ਰਵਏ',\n",
       " 'ਕੰਮ',\n",
       " 'ਮੁਸਲਮਾਨ',\n",
       " 'ਵਾਸੀ',\n",
       " 'ਪਿੰਡ',\n",
       " 'ਦੱਪ',\n",
       " 'ਬਲੇ',\n",
       " 'ਬਾਣਾ',\n",
       " 'ਦੀਗਲ',\n",
       " 'ਬੈਂਕ',\n",
       " 'ਜਿਲਾ',\n",
       " 'ਕਿਸ਼ਨਗੰਜ',\n",
       " 'ਬਿਹਾਰ',\n",
       " 'ਹਾਲ',\n",
       " 'ਵਾਸੀ',\n",
       " 'ਖੋਥੜਾ',\n",
       " 'ਮੁਹੱਲਾ',\n",
       " 'PS',\n",
       " 'ਸਿਟੀ',\n",
       " 'ਫਗਵਾੜਾ',\n",
       " 'ਜੋ',\n",
       " 'ਹਮ',\n",
       " 'ਏਕ',\n",
       " 'ਸਾਥ',\n",
       " 'ਹੀ',\n",
       " 'ਰਹਿਤੇ',\n",
       " 'ਖੇ,',\n",
       " 'ਮੋਟਰਸਾਈਕਲ',\n",
       " 'ਕੇ',\n",
       " 'ਪੀਛੇ',\n",
       " 'ਬੈਠਾ',\n",
       " 'ਥਾ',\n",
       " 'ਔਰ',\n",
       " 'ਮੈਂ',\n",
       " 'ਮੋਟਰਸਾਈਕਲ',\n",
       " 'ਚਲਾ',\n",
       " 'ਰਹਾ',\n",
       " 'ਸ਼ਾ।ਪੀਛੇ',\n",
       " 'ਸੇ',\n",
       " 'ਏਕ',\n",
       " 'ਮਰੂਤੀ',\n",
       " 'ਕਾਰ',\n",
       " 'ਜਿਸ',\n",
       " 'ਕਾ',\n",
       " 'ਨੰਬਰ',\n",
       " 'ਮੁਝੇ',\n",
       " 'ਯਾਦ',\n",
       " 'ਨਹੀਂ',\n",
       " 'ਹੈ।ਜੋ',\n",
       " 'ਹਮਾਰੇ',\n",
       " 'ਪੁੱਛੇ',\n",
       " 'ਸੌ',\n",
       " 'ਤੇਜ',\n",
       " 'ਰਫਰ',\n",
       " 'ਸੇ',\n",
       " 'ਆਇਆ',\n",
       " 'ਔਰ',\n",
       " 'ਹਮੇਂ',\n",
       " 'ਟੱਕਰ',\n",
       " 'ਮਾਰ',\n",
       " 'ਦੀ',\n",
       " 'ਇਸ',\n",
       " 'ਟੱਕਰ',\n",
       " 'ਮੋ',\n",
       " 'ਮੇਰੀ',\n",
       " 'ਬਾਈ',\n",
       " 'ਨਾਤ',\n",
       " 'ਟੁਟ',\n",
       " 'ਗਈ',\n",
       " 'ਔਰ',\n",
       " 'ਸਾਦਿਲ',\n",
       " 'ਕੇ',\n",
       " 'ਸਿਰ',\n",
       " 'ਮੇਂ',\n",
       " 'ਬਹੁਤ',\n",
       " 'ਗਹਿਰੀ',\n",
       " 'ਚੋਟ',\n",
       " 'ਆਈ।ਕਾਜ਼',\n",
       " 'ਵਾਲੇ',\n",
       " 'ਨੂੰ',\n",
       " 'ਹਮੇ',\n",
       " 'ਆਪਣੀ',\n",
       " 'ਕਾਰ',\n",
       " 'ਮੈਂ',\n",
       " 'ਬਿਨਾਇਆ',\n",
       " 'ਔਰ',\n",
       " 'ਸਿਵਲ',\n",
       " 'ਹਸਪਤਾਲ',\n",
       " 'ਵਿਲੇਜ਼',\n",
       " 'ਲੈ',\n",
       " 'ਗਿਆ',\n",
       " 'ਤਾਂ',\n",
       " 'ਡਾਕਟਰ',\n",
       " 'ਨੇ',\n",
       " 'ਮੇਰੀ',\n",
       " 'ਮਲਮ',\n",
       " 'ਪੱਟੀ',\n",
       " 'ਕੇ',\n",
       " 'ਅੱਚ',\n",
       " 'ਹਮ',\n",
       " 'ਦੋਨੋਂ',\n",
       " 'ਕੇ',\n",
       " 'ਐਬੁਲੈਂਸ',\n",
       " 'ਮੈਂ',\n",
       " 'ਬਿਨਾ',\n",
       " 'ਕਰ',\n",
       " 'ਹਾਂ',\n",
       " 'DMC',\n",
       " 'ਹਸਪਤਾਲ',\n",
       " 'LDH',\n",
       " 'ਵਿੱਚ',\n",
       " 'ਦਾਖਲ',\n",
       " 'ਕਰਵਾ',\n",
       " 'ਦੀਆ।ਇੱਥੇ',\n",
       " 'ਮੈਂ',\n",
       " 'ਜੋਰ',\n",
       " 'ਇਲਾਜ',\n",
       " 'ਹਾਂ!ਮਰੂਚੀ',\n",
       " 'ਕਾਰ',\n",
       " 'ਕਾ',\n",
       " 'ਨੱਥਰ',\n",
       " 'ਸੁਭਾ',\n",
       " 'ਪਤਾ',\n",
       " 'ਕਰਕੇ',\n",
       " 'ਆਪ',\n",
       " 'ਕੋ',\n",
       " 'ਬਤਾ',\n",
       " 'ਦੇ',\n",
       " 'ਮਰੂਤੀ',\n",
       " 'ਕਾਰ',\n",
       " 'ਵਾਲੇ',\n",
       " 'ਦੇ',\n",
       " 'ਖਿਲਾਫ',\n",
       " 'ਕਾਨੂੰਨੀ',\n",
       " 'ਕਾਰਵਾਈ',\n",
       " 'ਕੀ',\n",
       " 'ਜਾਏ।',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਲਿਖਵਾ',\n",
       " 'ਦੀਆ',\n",
       " 'ਹੈ।',\n",
       " 'Sd',\n",
       " 'ਹਿੰਦੀ',\n",
       " 'ਹਸਨੈਨ',\n",
       " 'ਤਸਦੀਕ',\n",
       " 'Sd',\n",
       " 'ਪੰਜਾਬੀ',\n",
       " 'ਨਿਰਮਲ',\n",
       " 'ਸਿੰਘ',\n",
       " 'ASI',\n",
       " 'ਖਾਣਾ',\n",
       " 'ਫਿਲੋਰ',\n",
       " '06.01.2019',\n",
       " 'ਕਾਰਵਾਈ',\n",
       " 'ਪੁਲਿਸ-',\n",
       " 'ਅੱਜ',\n",
       " 'ਮਨ',\n",
       " 'AS',\n",
       " 'ਹਾਜਰ',\n",
       " 'ਬਾਣਾ',\n",
       " 'ਹਾਂ',\n",
       " 'ਕਿ',\n",
       " 'MHC',\n",
       " 'ਰਾਣਾ',\n",
       " 'ਨੇ',\n",
       " 'ਨੋਟ',\n",
       " 'ਕਰਵਾਇਆ',\n",
       " 'ਕਿ',\n",
       " 'DMC',\n",
       " 'ਲੁਧਿਆਣਾ',\n",
       " 'ਤੋਂ',\n",
       " 'ਇੱਕ',\n",
       " 'ਫੈਨ',\n",
       " 'ਮੋਸੂਲ',\n",
       " 'ਹੋਇਆ',\n",
       " 'ਕਿ',\n",
       " 'ਦਿਲ',\n",
       " 'ਜੋ',\n",
       " 'ਰੰਡਮੋਕਸੀਫੈਟ',\n",
       " 'ਹੋਣ',\n",
       " 'ਕਰਕੇ',\n",
       " 'ਸਿਰ',\n",
       " 'ਦੀ',\n",
       " 'ਸੱਟ',\n",
       " 'ਕਾਰਨ',\n",
       " 'ਜੈਜ਼',\n",
       " 'ਇਲਾਜ',\n",
       " 'ਦੀ',\n",
       " 'ਮੌਤ',\n",
       " 'ਹੋ',\n",
       " 'ਚੁੱਕੀ',\n",
       " 'ਹੈ',\n",
       " 'ਹਸਪਤਾਲ',\n",
       " 'ਪੁੱਜ',\n",
       " 'ਕੇ',\n",
       " 'ਕਾਰਵਾਈ',\n",
       " 'ਕੀਤੀ',\n",
       " 'ਜਾਵੇ',\n",
       " 'ਜਿਸਤੇ',\n",
       " 'ਮਨ',\n",
       " 'ASI',\n",
       " 'ਸਮੇਤ',\n",
       " 'HC',\n",
       " 'ਸੰਜੀਵ',\n",
       " 'ਕੁਮਾਰ,',\n",
       " '417',\n",
       " 'PHG',\n",
       " 'ਨਵੀਰ',\n",
       " 'ਸਿੰਘ',\n",
       " 'ਨੇ',\n",
       " '2072',\n",
       " 'ਦੇ',\n",
       " 'ਬਚਾਏ',\n",
       " 'ਕਰਨੇ',\n",
       " 'ਕਾਰਵਾਈ',\n",
       " 'DMC',\n",
       " 'ਹਸਪਤਾਲ',\n",
       " 'ਲੁਧਿਆਣਾ',\n",
       " 'ਪੁੱਜ',\n",
       " 'ਕੇ',\n",
       " 'ਚੁੱਕਾ',\n",
       " 'ਬਰ',\n",
       " '56876',\n",
       " 'ਮਿਤੀ',\n",
       " '06.01.19',\n",
       " 'ਹਾਸਿਲ',\n",
       " 'ਕੀਤਾ',\n",
       " 'Dr.',\n",
       " 'ਸਾਜ਼ਿਬ',\n",
       " 'ਪਾਸ',\n",
       " 'ਮਜਬੂਬ',\n",
       " 'ਦਾ',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਲੈਣ',\n",
       " 'ਸਬੰਧੀ',\n",
       " 'ਲਿਖਤੀ',\n",
       " 'ਰਾਇ',\n",
       " 'ਹਾਸਲ',\n",
       " 'ਕੀਤੀ',\n",
       " 'Dr.',\n",
       " 'ਸਾਹਿਬ',\n",
       " 'ਨੇ',\n",
       " 'ਮਜਹੂ',\n",
       " 'ਨੂੰ',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਦੇਣ',\n",
       " 'ਦੇ',\n",
       " 'ਛਿੱਟ',\n",
       " 'ਲਿਖਿਆ',\n",
       " 'ਜਿਸਤੇ',\n",
       " 'ਹਸਨੈਨ',\n",
       " 'ਨੇ',\n",
       " 'ਮੇਰੇ',\n",
       " 'ਪਾਸ',\n",
       " 'ਆਪਣਾ',\n",
       " 'ਉਕਤ',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਦਰਜ',\n",
       " 'ਕਰਵਾਇਆ',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਲਿਖਕੇ',\n",
       " 'ਪੜਕੇ',\n",
       " 'ਸੁਣਾਇਆ',\n",
       " 'ਹੈ',\n",
       " 'ਸਮਝਾਇਆ',\n",
       " 'ਗਿਆ',\n",
       " 'ਜਿਸਨੇ',\n",
       " 'ਆਪਣੇ',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਨੂੰ',\n",
       " 'ਠੀਕ',\n",
       " 'ਮੰਨਤ',\n",
       " 'ਤੇ',\n",
       " 'ਹਿੰਦੀ',\n",
       " 'ਵਿੱਚ',\n",
       " 'ਆਪਣੇ',\n",
       " 'ਦਸਤਖਤ',\n",
       " 'ਕੀਤੇ',\n",
       " 'ਜਿਸਦੀ',\n",
       " 'ਮਨ',\n",
       " 'ASI',\n",
       " 'ਨੇ',\n",
       " 'ਤਸਦੀਕ',\n",
       " 'ਕੀਤੀ',\n",
       " 'ਬਿਆਨ',\n",
       " 'ਬਾਲਾਂ',\n",
       " 'ਦੇ',\n",
       " 'ਜੁਰਮ',\n",
       " '304,279,',\n",
       " '338,337',\n",
       " 'ਕੁਦ',\n",
       " 'ਦਾ',\n",
       " 'ਹੋਣਾ',\n",
       " 'ਪਾਇਆ',\n",
       " 'ਜਾਂਦਾ',\n",
       " 'ਹੈ',\n",
       " 'ਜਿਸਚੇ',\n",
       " 'ਚੁੱਕਾ',\n",
       " 'ਹਜ਼ਾ',\n",
       " 'ਲਿਖਕੇ',\n",
       " 'ਮੁਕੱਦਮਾ',\n",
       " 'ਦਰਜ',\n",
       " 'ਰਜਿਸਟਰ',\n",
       " 'ਕਰਨ',\n",
       " 'ਲਈ',\n",
       " 'ਹੱਥੀ',\n",
       " 'PHG',\n",
       " 'ਧੰਨਵੀਰ',\n",
       " 'ਸਿੰਘ',\n",
       " '282',\n",
       " 'ਦੇ',\n",
       " 'ਥਾਣਾ',\n",
       " 'ਭਜਿਆ',\n",
       " 'ਜਾਂਦਾ',\n",
       " 'ਹੈ',\n",
       " 'ਮੁਕੱਦਮਾ',\n",
       " 'ਦਰਜ',\n",
       " 'ਕਰਕੇ',\n",
       " 'ਨੰਬਰ',\n",
       " 'ਮੁਕੱਦਮਾ',\n",
       " 'ਤੇ',\n",
       " 'ਜਾ',\n",
       " 'ਕੀਤਾ',\n",
       " 'ਜਾਵੇ',\n",
       " 'ਕਟਲ',\n",
       " 'ਰੂਮ',\n",
       " 'ਪਰ',\n",
       " 'ਬਜਰੀਆ',\n",
       " 'ਵਾਲਿਰਲੈਸ',\n",
       " 'ਇਤਲਾਹ',\n",
       " 'ਦਿੱਤੀ',\n",
       " 'ਜਾਵੇ',\n",
       " 'ਮਨ',\n",
       " 'AS।',\n",
       " 'ਸਮੇਤ',\n",
       " 'ਸਾਥੀ',\n",
       " 'ਕਰਮਚਾਰੀ',\n",
       " 'ਦੇ',\n",
       " 'ਮੌਕਾ',\n",
       " 'ਪਰ',\n",
       " 'ਮਸ਼ਰੂਫ',\n",
       " 'ਭਵਤੀ',\n",
       " 'ਹਾਂ',\n",
       " 'Sd',\n",
       " 'ਪੰਜਾਬੀ',\n",
       " 'ਨਿਰਮਲ',\n",
       " 'ਸਿੰਘ',\n",
       " 'ASI',\n",
       " 'ਬਾਣਾ',\n",
       " 'ਫਿਲੋਰ',\n",
       " 'ਜਲੰਧਰ',\n",
       " 'ਦਿਹਾਤੀ',\n",
       " '06.01.19',\n",
       " 'ਅੱਜ',\n",
       " 'ਸ਼ਾਹੱਦ',\n",
       " 'ਰਕਬਾ',\n",
       " 'DMC',\n",
       " 'ਹਸਪਤਾਲ',\n",
       " 'ਲੁਧਿਆਣਾ',\n",
       " 'AT',\n",
       " '10:40',\n",
       " 'PM',\n",
       " 'Longitude:75,784626,',\n",
       " 'Latitude:',\n",
       " '31.030583',\n",
       " 'ਅੱਜ',\n",
       " 'ਖਾਣਾ:-ਉਪਰੋਕਤ',\n",
       " 'ਲਿਖ',\n",
       " 'ਦੇ',\n",
       " 'ਜਾਣਾ',\n",
       " 'ਮਮੂਲ',\n",
       " 'ਹੋਣ',\n",
       " 'ਤੇ',\n",
       " 'ਇਹ',\n",
       " 'ਪਹਿਲੀ',\n",
       " 'ਸੂਚਨਾ',\n",
       " 'ਰਿਪੋਰਟ',\n",
       " 'ਉਕਤ',\n",
       " 'ਧਾਰਾ',\n",
       " 'ਹੇਨ',\n",
       " 'ਦਰਜ',\n",
       " 'ਰਜਿਸਟਰ',\n",
       " 'ਕਰਕੇ',\n",
       " 'ਅਸਲ',\n",
       " 'ਲਿਖਤ',\n",
       " 'ਮਹਿ',\n",
       " 'ਥਲ',\n",
       " 'FIR',\n",
       " 'ਹੱਥੀ',\n",
       " 'PHG',\n",
       " 'ਅਰਿੰਦਾ',\n",
       " 'ਨਿਯਦ',\n",
       " 'ASI',\n",
       " 'ਪਾਸ਼',\n",
       " 'ਬਰ',\n",
       " 'ਮੌਕਾ',\n",
       " 'ਏ',\n",
       " 'ਫੜੀ',\n",
       " 'ਭੇਜੀ',\n",
       " 'ਜਾ',\n",
       " 'ਰਹੀ',\n",
       " 'ਹੈ।',\n",
       " 'ਮੁੱਖ',\n",
       " 'ਅਫਸਰ',\n",
       " 'ਥਾਣਾ',\n",
       " 'ਅਤੇ',\n",
       " 'ਕੰਟਰੋਲ',\n",
       " 'ਰੂਮ',\n",
       " 'ਪਰ',\n",
       " 'ਬਜ਼ਰੀਆ',\n",
       " 'ਵਾਇਰਲੇਸ',\n",
       " 'ਇਤਲਾਹ',\n",
       " 'ਭੇਜੀ',\n",
       " 'ਜਾ',\n",
       " 'ਰਹੀ',\n",
       " 'ਹੈ।']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"test\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(data[\"test\"], size=100, window=5, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.8271695e-04,  5.6593148e-05,  4.4056182e-03, -7.8639976e-05,\n",
       "        1.1065550e-03,  6.3611165e-04, -4.6216496e-03, -1.8147661e-03,\n",
       "       -8.5680455e-04, -4.2598443e-03, -1.7400404e-03,  1.8875590e-03,\n",
       "        7.2091463e-04, -1.2150854e-03, -4.4008205e-03,  2.5993471e-03,\n",
       "        1.4890076e-03, -4.8427763e-03,  5.9638993e-04, -1.5284579e-03,\n",
       "        1.7272736e-03, -3.3339642e-03,  1.0421427e-03,  4.8363185e-03,\n",
       "        3.0959551e-03, -2.9497519e-03,  4.1424134e-03, -1.5093917e-03,\n",
       "        8.3504204e-04,  2.6154197e-03,  2.8199058e-03, -2.9046873e-03,\n",
       "        1.0323739e-04,  4.2139823e-03,  3.3927332e-03,  3.6553589e-03,\n",
       "        1.1144144e-03,  4.4277971e-05,  3.7618154e-03, -4.5367358e-03,\n",
       "       -4.5358427e-03, -4.2216090e-04, -3.5798328e-03,  3.7605548e-03,\n",
       "        3.9221894e-04, -1.0150195e-03,  1.7728851e-03, -2.2757971e-03,\n",
       "       -1.9580892e-03, -4.8958324e-04, -1.7418992e-03,  2.0089271e-03,\n",
       "       -3.0523591e-04, -9.7025686e-04, -2.5783498e-03,  2.6524391e-03,\n",
       "        3.5106929e-03, -2.6765125e-04,  3.6314912e-03,  3.4280107e-04,\n",
       "        1.9142848e-03, -3.1145616e-03, -5.0086137e-03,  4.1942503e-03,\n",
       "       -2.2473896e-03,  4.6092025e-03, -3.1589349e-03,  4.2307577e-03,\n",
       "       -3.3275681e-04,  3.1612390e-03, -3.2081162e-03,  8.3531917e-04,\n",
       "       -2.8668658e-03,  4.0564425e-03,  3.7360848e-03, -4.5496128e-03,\n",
       "       -4.7069900e-03,  1.3497971e-03, -9.2172745e-04, -3.6116731e-03,\n",
       "        4.0694755e-03, -3.7392688e-03,  4.8503927e-03, -2.2954481e-04,\n",
       "       -2.6599097e-03,  1.5421434e-03,  3.7256028e-03, -4.9871081e-03,\n",
       "       -1.2090090e-03, -8.4527628e-04, -2.3070383e-03,  2.1512520e-03,\n",
       "       -2.1321001e-03,  4.1697752e-03,  2.1287331e-03,  3.1070730e-03,\n",
       "        1.7134314e-03,  4.9540014e-03,  2.6114257e-03, -2.3815624e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectors[\"ਨੂੰ\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import FastText\n",
    "#model_ted = FastText(data[\"test\"], size=100, window=5, min_count=5, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-4491a9b510fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         }\n\u001b[0;32m     13\u001b[0m \u001b[0mtfidf\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize(p):\n",
    "    p = [tokenize(doc) for doc in p]\n",
    "    texts  = TextCollection(p)\n",
    "\n",
    "    for doc in p:\n",
    "        yield {\n",
    "            term: texts.tf_idf(term, doc)\n",
    "            for term in doc\n",
    "        }\n",
    "tfidf  = TfidfVectorizer()\n",
    "p = tfidf.fit_transform(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3_again\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-081e5fd21174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, corpus_file, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, docvecs, docvecs_mapfile, comment, trim_rule, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You can't pass a generator as the documents argument. Try a sequence.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             self.train(\n\u001b[0;32m    617\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m   1182\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m   1183\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m         )\n\u001b[0;32m   1186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         logger.info(\n",
      "\u001b[1;32m~\\anaconda3_again\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m                     logger.warning(\n\u001b[0;32m   1312\u001b[0m                         \u001b[1;34m\"Each 'words' should be a list of words (usually unicode strings). \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "#p = [list(tokenize(doc)) for doc in p]\n",
    "#p = [\n",
    " #   TaggedDocument(words, ['d{}'.format(idx)])\n",
    "  #  for idx, words in enumerate(corpus)\n",
    "#]\n",
    "\n",
    "model = Doc2Vec(p, size=5, min_count=0)\n",
    "print(model.docvecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
